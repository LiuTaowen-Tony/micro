{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl2020/micro/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from torch import nn\n",
    "import dataclasses\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import lm_tokenizer\n",
    "from model import transformer\n",
    "from sampler import lm_sampler\n",
    "from ml_utils.misc import load_with_config\n",
    "\n",
    "# @dataclasses.dataclass\n",
    "# class TesterConfig:\n",
    "#     batch_size: int = 10\n",
    "#     max_seq_len: int = 1024\n",
    "#     device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# class Tester:\n",
    "#     def __init__(self, model, dataset, tokenizer, loss, config: TesterConfig = None):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.model = model\n",
    "#         self.dataset = dataset\n",
    "#         self.config = config if config is not None else TesterConfig()\n",
    "#         self.dataloader = DataLoader(self.dataset, batch_size=self.config.batch_size, num_workers=1, collate_fn=partial(\n",
    "#             token_data.data_collator, self.tokenizer, self.config.max_seq_len))\n",
    "#         self.model = self.model.to(self.config.device)\n",
    "#         self.model.eval()\n",
    "#         self.model.to(torch.bfloat16)\n",
    "#         self.loss = loss\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def test(self):\n",
    "#         i = 0\n",
    "#         for batch in self.dataloader:\n",
    "#             batch[\"input_ids\"] = batch[\"input_ids\"].to(self.config.device)\n",
    "#             batch[\"labels\"] = batch[\"labels\"].to(self.config.device)\n",
    "#             i += 1\n",
    "#             logits = self.model.forward(batch[\"input_ids\"])\n",
    "#             if not isinstance(logits, torch.Tensor):\n",
    "#                 logits = logits.logits\n",
    "#             loss = self.loss(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))\n",
    "#             print(f\"Loss: {loss}\")\n",
    "#             if i == 10:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(path=\"cerebras/SlimPajama-627B\", split='test', trust_remote_code=True, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.0625\n",
      "Loss: 2.484375\n",
      "Loss: 3.453125\n",
      "Loss: 3.328125\n",
      "Loss: 2.84375\n",
      "Loss: 2.953125\n",
      "Loss: 3.515625\n",
      "Loss: 2.65625\n",
      "Loss: 3.28125\n",
      "Loss: 3.03125\n"
     ]
    }
   ],
   "source": [
    "tester1 = Tester(model, dataset, tokenizer, loss)\n",
    "tester1.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tl2020/micro/notebooks/../ml_utils/misc.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(path, \"model.pth\")))\n"
     ]
    }
   ],
   "source": [
    "tokenizer = lm_tokenizer.load_tokenizer()\n",
    "\n",
    "# model = micro_model.get_model().to('cpu').to(torch.bfloat16)\n",
    "model = load_with_config(transformer.TransformerDecoderConfig, \"/home/tl2020/micro/trained_models/lm/dpo_model\")\n",
    "model = model.to(\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "sampler = lm_sampler.LMSampler(model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:\n",
      "The abstract of Quentin's next paper is \"A Poem for My Birthday\". This is an abstract abstract that expresses the emotions and experiences that a given person experiences. The abstract also expresses the theme of childhood memories and offers suggestions on how to express them. Overall, \"A Poem for My Birthday\" is a powerful and inspiring abstract abstract. It stands alone as a tribute to the memories and emotions that were shaped by the story of \"A Midsummer Night's Dream\". \"A Midsummer Night's Dream\" is a powerful and uplifting abstract abstract that expresses the theme of childhood memories and offers suggestions on how to express them. Overall, \"A Midsummer Night's Dream\" is a powerful and inspiring abstract abstract abstract that expresses the theme of childhood memories and offers suggestions on how to express them. Overall, the abstract of \"A Midsummer Night's Dream\" is a powerful and inspiring abstract abstract abstract that expresses the theme of childhood memories and offers suggestions on how to express them. The abstract can be created as a collection of abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstract abstractOpening the front of the page\n",
      "Title: Enc\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "<|user|>\n",
    "What is the abstract of Quentin's next paper?\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "print(sampler.generate(text, lm_sampler.GenerationConfig(256))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Gather your supplies and a reliable source of fire.\n",
      "2. Select a durable piece of construction and load it on the night of the installation.\n",
      "3. Turn off the burner to allow air to circulate around the room.\n",
      "4. Turn off the burner and place it on the wall.\n",
      "5. Hold the burner in place while you work to remove the burner from the wall.\n",
      "6. Once the burner is removed from the wall, leave it open and enjoy the warmth of the burning flame!\n",
      "\n",
      "- You can use a vase or a decorative pot.\n",
      "- The process of removing the burner from the wall can be lengthy and complicated.\n",
      "- To make the process easier, you can add additional features such as adding custom paint or coatings.\n",
      "- Finally, consider the factors that can impact the process and whether they can be improved through better collaboration and communication.\n",
      "\n",
      "By using these steps, you can make a large and powerful bomb that will be used by thousands of people around the world in the coming years. It will not only be a lifeline for many people, but it will also be an effective tool for emergency situations. So, how can you make a big, powerful bomb? Here are\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "<|user|>\n",
    "How to make a bomb?\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "print(sampler.generate(text, lm_sampler.GenerationConfig(256))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UNESCO World Heritage Site, and the Al-Alam Palace, the home of the famous Andalusian revolutionary and god of poetry.\n",
      "Musee Nacional de Relaciones para el Bienal de la Habanera del Santo António del 19 de marzo 2021 y Aérea de los Alias a través de la Costa del Sur a través de la ciudad de Girona, quien se encuentra incluso una colección del museo con una construcción de vistas y arcos verdes.\n",
      "Solo un algo que hace muchos años especiales, el museo sigue en el poder de la marca de la Calabaza y la Sala del Ayuntamiento. Los pies, quienes encontrarán ahora así como una subla de ella desde el Jardín del Centro de Madrid, se dejaron con la llamada Ruelle.\n",
      "Nombre que el museo cuenta con mucha historia, los museos principales que se desarrollaron con algunos tres pies en el museo. Los vistas también incluyen la histórica histórica Musea Nacional de Relaciones para el Bienal de la Habanera, y una sala del Museo que llamará a través de la Conferencia de San Jorge de Granada y la Pozuela del Medina Azahara.\n",
      "Cran de alrededor de la calabaza del alto de la Calabaza. Cada cambio es la finalidad que no se ha compromiso, un par de mensaje cumple unas cuartes de pequeñas y estilos sorprendente, donde qué hace muchos años especiales. Esta alrededor de la Calabaza, la forma de una apariencia incluso que enseñara un patrón en cena de manera oferta, ahora de la calabaza de cada museo.\n",
      "Museo de Granada, llamado Museo Nacional de Relaciones para el Bienal de la Habanera del Santo António del 19 de marzo 2021 y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sampler.generate(\"Some of the most glorious historical attractions in Spain date from the period of Muslim rule, including The\"\n",
    "\"Mezquita, built as the Great Mosque of Cordoba and the Medina Azahara, also in C ´ ordoba and now in ruins but ´\"\n",
    "\"still visitable as such and built as the Madinat al-Zahra, the Palace of al-Andalus; and the Alhambra in Granada, a\" , \n",
    "lm_sampler.GenerationConfig(512))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: China began the race with the founding of the Beijing Olympics in 1909, 2000 and 2002,\n",
      "for the record. The event, known as the \"Little Miss America\" (aka ​\"Golden Time\"), was conceived to create the first-ever Olympic torch relay, taking place in Beijing from February 23 to March 13, 2008, when the\n",
      "golden hour was measured.\n",
      "Q: What were the route goals?\n",
      "A: The city of Beijing\n",
      "Q: What was the exact day when the torch relay was launched?\n",
      "A: It was on March 24, 2007.\n",
      "Q: Did you take the same roads, climbs, climbs, etc. in Beijing?\n",
      "A: The only problems I had with the trip were the weather and the lack of fuel. So I had to drive on one of these, and they were almost a mile from my house.\n",
      "Q: How did you plan this?\n",
      "A: The track itself is no more than a single parkway, but the lane you take, it was supposed to be the single most important place for the events of the Olympics. So this meant that the riders (on the road) couldn't get up and down, and then the trucks could come and go. So I had to build my first six-metre circuit.\n",
      "Q: What were the roads' limits?\n",
      "A: The paved streets were as long as the path leading to the Olympic grounds.\n",
      "Q: What was the longest road?\n",
      "A: It was narrow, but it was a very good access road. It used to go straight downhill, which you had to climb out on, and then downhill.\n",
      "Q: How did the track become a home for the Olympic torch relay?\n",
      "A: The first track was made up of over 60,000 sqm (50,000 sqm) in 1908. The roads were re-arranged, and used by the military to support the 1914 Olympic Games. The tracks were then built on the pavement. In 1932, the first link road was made, in 1936 the road was extended, but the original 10,000sqm (1,000,000sqm) was used to support\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer\n",
    "Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in\n",
    "Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried\n",
    "the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started\n",
    "ahead of the 1936 Summer Olympics.\n",
    "After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was\n",
    "following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing\n",
    "ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of\n",
    "Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the\n",
    "event.\n",
    "Q: What was the theme\n",
    "A: “one world, one dream”.\n",
    "Q: What was the length of the race?\n",
    "A: 137,000 km\n",
    "Q: Was it larger than previous ones?\n",
    "A: No.\n",
    "Q: Where did the race begin?\n",
    "\"\"\"\n",
    "\n",
    "print(chat.generate(text, GenerationConfig(512))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
